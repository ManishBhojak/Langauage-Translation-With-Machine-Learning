{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dc01fb",
   "metadata": {},
   "source": [
    "# Language Translation with Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939330fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=pd.read_csv('Hindi_English_Truncated_Corpus.csv', encoding=\"utf-8\")\n",
    "print(lines.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[lines['source']=='ted']\n",
    "lines=lines[~pd.isnull(lines['english_sentence'])]\n",
    "lines.drop_duplicates(inplace=True)\n",
    "\n",
    "#Lets pick up any 25000 rows from the dataset\n",
    "lines = lines.sample(n=25000, random_state=42)\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.lower())\n",
    "lines['hindi_sentence']= lines['hindi_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special Characters\n",
    "# Remove all the special Characters\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x : x.translate(remove_digits))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\",\"\",x))\n",
    "\n",
    "#Remove extra spaces\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.strip())\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.strip())\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\" +\",\" \", x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'START_ '+x+' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067caeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get English and Hindi Vocabulary\n",
    "all_eng_words= set()\n",
    "for eng in lines['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "             all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words = set()\n",
    "for hin in lines['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)\n",
    "lines['length_eng_sentence'] = lines['english_sentence'].apply(lambda x: len(x.split(\" \")))\n",
    "lines['length_hin_sentence'] = lines['hindi_sentence'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=lines[lines['length_eng_sentence']<=20]\n",
    "lines=lines[lines['length_hin_sentence']<=20]\n",
    "max_length_src = max(lines['length_hin_sentence'])\n",
    "max_length_tar = max(lines['length_eng_sentence'])\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "\n",
    "num_decoder_tokens +=1 #for zero padding\n",
    "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "reverse_input_char_index = dict((i,word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "lines = shuffle(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe05d72",
   "metadata": {},
   "source": [
    "## Training Model to Translate English to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X= X_train, y= Y_train, batch_size= 120):\n",
    "    '''Generate a batch of data'''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
    "            # Pass both X and y slices to zip to get pairs of English and Hindi sentences\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    # Handle cases where word might not be in input_token_index (though unlikely with current preprocessing)\n",
    "                    if word in input_token_index:\n",
    "                         encoder_input_data[i,t] = input_token_index[word] #encoder input seq\n",
    "                for t,word in enumerate(target_text.split()):\n",
    "                    # Handle cases where word might not be in target_token_index (though unlikely with current preprocessing)\n",
    "                    if word in target_token_index:\n",
    "                        if t<len(target_text.split())-1:\n",
    "                            decoder_input_data[i, t] = target_token_index[word] #decoder input seq\n",
    "                        if t>0:\n",
    "                            #decoder target sequence (one hot encoded)\n",
    "                            #does not include the START_ token\n",
    "                            #Offset by one timestep\n",
    "                            decoder_target_data[i,t-1, target_token_index[word]] = 1.\n",
    "            # Change the yielded input from a list to a tuple\n",
    "            yield((encoder_input_data, decoder_input_data), decoder_target_data)\n",
    "\n",
    "latent_dim=300\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero= True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "#We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states\n",
    "#return states in the training model, but we will use them in inference\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#Define the model\n",
    "#`encoder_input_data` and `decoder_input_data` are the inputs\n",
    "# The model definition is already correct, expecting a list of inputs based on the Input layers.\n",
    "# The issue is how the generator provides these inputs.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 120\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generate_batch(X_train, Y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch= train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_batch(X_test, Y_test, batch_size),\n",
    "                    validation_steps= val_samples//batch_size)\n",
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e067ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to geet the \"thought Vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "#Decoder setup\n",
    "#Below Tensors will hold the states of the previous time step\n",
    "decoder_state_input_h= Input(shape=(latent_dim,))\n",
    "decoder_state_input_c= Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb2=dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "#To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "#Final decoder Model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs\n",
    "    [decoder_outputs2] + decoder_states2\n",
    ")\n",
    "def decode_sequence(input_seq):\n",
    "    #Encode the input as state vectors.\n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "    #Generate empty target sequence of length 1.\n",
    "    target_seq =np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0,0] = target_token_index['START_']\n",
    "    \n",
    "    #Sampling loop for a batch of sequences\n",
    "    #(to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, 1])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence+= ' '+sampled_char\n",
    "        \n",
    "        #Exit condition either hit max length\n",
    "        #or find stop character\n",
    "        if(sampled_char == '_END' or len(decoded_sentence)> 50):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence\n",
    "train_gen = generate_batch(X_train, Y_train, batch_size=1)\n",
    "k=-1\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English Sequence: ', X_train[k: k+1].values[0])\n",
    "print('Actual Hindi Translation: ', Y_train[k: k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation: ', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
